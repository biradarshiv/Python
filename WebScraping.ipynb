{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1efa67d5-fad8-4aaa-9e70-8d9b6466d9cc",
   "metadata": {},
   "source": [
    "FIRST BUILD A SAMPLE AND THEN WORK ON Arani DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3375095-9dc7-401a-a683-15a6adbc5859",
   "metadata": {},
   "source": [
    "Data scraping in Python, often referred to as web scraping when dealing with websites, involves extracting information from various sources, typically web pages. This process usually follows a series of steps:\n",
    "Identify the Target Data and Source:\n",
    "Determine what information needs to be extracted and from which website or data source. This involves understanding the structure of the source (e.g., HTML for websites, JSON for APIs).\n",
    "\n",
    "Choose Appropriate Libraries:\n",
    "\n",
    "requests: For making HTTP requests to retrieve the content of web pages.\n",
    "BeautifulSoup4 (bs4): For parsing HTML and XML documents, making it easy to navigate and search the parsed content.\n",
    "Selenium: For scraping dynamic websites that rely heavily on JavaScript, as it can simulate browser interactions.\n",
    "lxml: A fast and powerful library for parsing XML and HTML, often used with XPath expressions for data extraction.\n",
    "pandas: For storing and manipulating the extracted data in a structured format (e.g., DataFrames).\n",
    "\n",
    "Fetch the Data:\n",
    "Use a library like requests to send an HTTP GET request to the target URL and retrieve the raw content (e.g., HTML).\n",
    "Python\n",
    "\n",
    "    import requests\n",
    "    url = \"https://example.com\"\n",
    "    response = requests.get(url)\n",
    "    html_content = response.text\n",
    "    \n",
    "Parse the Data: If dealing with HTML, use a parsing library like BeautifulSoup to create a parse tree from the raw content. This allows for easy navigation and searching of elements.\n",
    "Python\n",
    "\n",
    "    from bs4 import BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    \n",
    "Extract Specific Data: Use methods provided by the parsing library (e.g., find(), find_all(), select(), XPath expressions) to locate and extract the desired elements and their content based on their tags, attributes, or other characteristics.\n",
    "Python\n",
    "\n",
    "    # Example: Extracting all links\n",
    "    links = soup.find_all(\"a\")\n",
    "    for link in links:\n",
    "        print(link.get(\"href\"))\n",
    "\n",
    "    # Example: Extracting text from a specific element by class\n",
    "    title_element = soup.find(\"h1\", class_=\"main-title\")\n",
    "    if title_element:\n",
    "        print(title_element.text)\n",
    "        \n",
    "Store the Extracted Data: Organize the extracted data into a suitable format, such as a list of dictionaries, a pandas DataFrame, or directly save it to a file (e.g., CSV, JSON, Excel).\n",
    "Python\n",
    "\n",
    "    import pandas as pd\n",
    "    data = []\n",
    "    # Populate 'data' with extracted information\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(\"extracted_data.csv\", index=False)\n",
    "    \n",
    "Handle Pagination and Dynamic Content (if necessary): If the data spans multiple pages or requires user interaction, implement logic to navigate through pages (e.g., by modifying URLs or using Selenium to simulate clicks) and handle dynamically loaded content.\n",
    "\n",
    "Important Considerations:\n",
    "Legality and Ethics:\n",
    "Always check the website's robots.txt file and terms of service before scraping. Respect rate limits and avoid overwhelming servers.\n",
    "\n",
    "Error Handling:\n",
    "Implement robust error handling for network issues, missing elements, or unexpected page structures.\n",
    "\n",
    "Website Changes:\n",
    "Be aware that websites can change their structure, which may break the scraping script. Regular maintenance of the script may be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e89aa03-33ad-4897-ba6c-677b75ba5738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = \"https://example.com\"\n",
    "response = requests.get(url)\n",
    "html_content = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d7925a3-aa0c-4a6d-89df-ac8e342cab39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!doctype html>\n",
      "<html>\n",
      "<head>\n",
      "    <title>Example Domain</title>\n",
      "\n",
      "    <meta charset=\"utf-8\" />\n",
      "    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\" />\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n",
      "    <style type=\"text/css\">\n",
      "    body {\n",
      "        background-color: #f0f0f2;\n",
      "        margin: 0;\n",
      "        padding: 0;\n",
      "        font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
      "        \n",
      "    }\n",
      "    div {\n",
      "        width: 600px;\n",
      "        margin: 5em auto;\n",
      "        padding: 2em;\n",
      "        background-color: #fdfdff;\n",
      "        border-radius: 0.5em;\n",
      "        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\n",
      "    }\n",
      "    a:link, a:visited {\n",
      "        color: #38488f;\n",
      "        text-decoration: none;\n",
      "    }\n",
      "    @media (max-width: 700px) {\n",
      "        div {\n",
      "            margin: 0 auto;\n",
      "            width: auto;\n",
      "        }\n",
      "    }\n",
      "    </style>    \n",
      "</head>\n",
      "\n",
      "<body>\n",
      "<div>\n",
      "    <h1>Example Domain</h1>\n",
      "    <p>This domain is for use in illustrative examples in documents. You may use this\n",
      "    domain in literature without prior coordination or asking for permission.</p>\n",
      "    <p><a href=\"https://www.iana.org/domains/example\">More information...</a></p>\n",
      "</div>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ef4a3e1-6c6f-47b2-bd43-2f6228a5a7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f476dee-61b5-4932-a4bd-106b8d910aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "\n",
      "<html>\n",
      "<head>\n",
      "<title>Example Domain</title>\n",
      "<meta charset=\"utf-8\"/>\n",
      "<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-type\"/>\n",
      "<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
      "<style type=\"text/css\">\n",
      "    body {\n",
      "        background-color: #f0f0f2;\n",
      "        margin: 0;\n",
      "        padding: 0;\n",
      "        font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
      "        \n",
      "    }\n",
      "    div {\n",
      "        width: 600px;\n",
      "        margin: 5em auto;\n",
      "        padding: 2em;\n",
      "        background-color: #fdfdff;\n",
      "        border-radius: 0.5em;\n",
      "        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\n",
      "    }\n",
      "    a:link, a:visited {\n",
      "        color: #38488f;\n",
      "        text-decoration: none;\n",
      "    }\n",
      "    @media (max-width: 700px) {\n",
      "        div {\n",
      "            margin: 0 auto;\n",
      "            width: auto;\n",
      "        }\n",
      "    }\n",
      "    </style>\n",
      "</head>\n",
      "<body>\n",
      "<div>\n",
      "<h1>Example Domain</h1>\n",
      "<p>This domain is for use in illustrative examples in documents. You may use this\n",
      "    domain in literature without prior coordination or asking for permission.</p>\n",
      "<p><a href=\"https://www.iana.org/domains/example\">More information...</a></p>\n",
      "</div>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3a341bc-9387-42ac-aa01-827191a5b33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.iana.org/domains/example\n",
      "h1 with class - has no data with class\n",
      "h1 without class\n",
      "Example Domain\n"
     ]
    }
   ],
   "source": [
    "# Example: Extracting all links\n",
    "links = soup.find_all(\"a\")\n",
    "for link in links:\n",
    "    print(link.get(\"href\"))\n",
    "\n",
    "# Example: Extracting text from a specific element by class\n",
    "print('h1 with class - has no data with class')\n",
    "title_element = soup.find(\"h1\", class_=\"main-title\")\n",
    "if title_element:\n",
    "    print(title_element.text)\n",
    "\n",
    "# Example: Extracting text from a specific element by class\n",
    "print('h1 without class')\n",
    "title_element = soup.find(\"h1\")\n",
    "if title_element:\n",
    "    print(title_element.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e74689c-203f-43ec-ba49-fe9da5646f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = []\n",
    "# Populate 'data' with extracted information\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"extracted_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d4be1a-79b2-4add-81eb-ae170f5c699f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f918dc3-7af6-44d9-81ca-8f5607b83b2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0afa56-e711-406a-8b25-e265fe6ec840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4d3fdf-8e71-440a-8da2-698d2f93bb52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922a21f6-c943-4e22-b22c-972c16b17f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Arani WORK STARTS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7e8879-00b7-4a39-84fe-5d7c72eb71c7",
   "metadata": {},
   "source": [
    "selenium with chrome driver\n",
    "Chrome version 138.0.7204.184"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "38332d55-4291-4d10-bdbb-01bfd0d7691c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.34.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting urllib3~=2.5.0 (from urllib3[socks]~=2.5.0->selenium)\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting trio~=0.30.0 (from selenium)\n",
      "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket~=0.12.2 (from selenium)\n",
      "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting certifi>=2025.6.15 (from selenium)\n",
      "  Downloading certifi-2025.7.14-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting typing_extensions~=4.14.0 (from selenium)\n",
      "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting websocket-client~=1.8.0 (from selenium)\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting attrs>=23.2.0 (from trio~=0.30.0->selenium)\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: sortedcontainers in /opt/anaconda3/lib/python3.11/site-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.11/site-packages (from trio~=0.30.0->selenium) (3.4)\n",
      "Collecting outcome (from trio~=0.30.0->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from trio~=0.30.0->selenium) (1.3.0)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.12.2->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /opt/anaconda3/lib/python3.11/site-packages (from urllib3[socks]~=2.5.0->selenium) (1.7.1)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.12.2->selenium)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Downloading selenium-4.34.2-py3-none-any.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading certifi-2025.7.14-py3-none-any.whl (162 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.7/162.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m754.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.8/129.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: websocket-client, urllib3, typing_extensions, h11, certifi, attrs, wsproto, outcome, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: websocket-client\n",
      "    Found existing installation: websocket-client 0.58.0\n",
      "    Uninstalling websocket-client-0.58.0:\n",
      "      Successfully uninstalled websocket-client-0.58.0\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.7\n",
      "    Uninstalling urllib3-2.0.7:\n",
      "      Successfully uninstalled urllib3-2.0.7\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2024.2.2\n",
      "    Uninstalling certifi-2024.2.2:\n",
      "      Successfully uninstalled certifi-2024.2.2\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.1.0\n",
      "    Uninstalling attrs-23.1.0:\n",
      "      Successfully uninstalled attrs-23.1.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "botocore 1.31.64 requires urllib3<2.1,>=1.25.4; python_version >= \"3.10\", but you have urllib3 2.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed attrs-25.3.0 certifi-2025.7.14 h11-0.16.0 outcome-1.3.0.post0 selenium-4.34.2 trio-0.30.0 trio-websocket-0.12.2 typing_extensions-4.14.1 urllib3-2.5.0 websocket-client-1.8.0 wsproto-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3a2820-b313-4536-9451-41c04bf6cd0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8be9e46b-34bc-443a-9843-a6ddd324c725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting webdriver-manager\n",
      "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.11/site-packages (from webdriver-manager) (2.31.0)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.11/site-packages (from webdriver-manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.11/site-packages (from webdriver-manager) (23.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests->webdriver-manager) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests->webdriver-manager) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests->webdriver-manager) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests->webdriver-manager) (2025.7.14)\n",
      "Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Installing collected packages: webdriver-manager\n",
      "Successfully installed webdriver-manager-4.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fd4585-8b31-4406-a06a-c0c878ac79f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wait for Dynamic Content (if necessary): If the data loads dynamically, you might need to use WebDriverWait and ExpectedConditions to wait for the specific elements containing the data to become present or visible.\n",
    "Extract Data with Beautiful Soup (or Selenium's built-in methods): Once the page is loaded, you can extract the HTML content using driver.page_source and then use Beautiful Soup or Selenium's own element-finding methods (find_element, find_elements) to locate and extract the data you need.\n",
    "Process and Store the Data: Organize the extracted data into a structured format (e.g., a Pandas DataFrame or a list of dictionaries) and save it to a file (like CSV, JSON, or a database). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ec34a3ee-3f62-487d-92e4-e379c0aec6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "driver has been quit\n",
      "2025-08-01 15:53:30.030754\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "driver.quit()\n",
    "print(f\"driver has been quit\")\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c5c26b-5a52-440e-a5aa-edeeace98ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fire in the hole!!, login and open the page within 120 seconds\n",
      "Found 203 elements with ID starting with 'opportunities-applied-manager-list-item-layout-'.\n",
      "\n",
      "DataFrame:\n",
      "                       HiringManagerName\n",
      "0          Hanrahan, FrankHiring Manager\n",
      "1            Doherty, AlanHiring Manager\n",
      "2  Srinivasan, KirubakaranHiring Manager\n",
      "3  Narasimhan, DhananjayanHiring Manager\n",
      "4  Narasimhan, DhananjayanHiring Manager\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n#FOLLOWING TWO HAS NOT WORKED\\n\\n# Example: Waiting for all elements with a specific class name to be visible\\n# Replace \\'your_class_name\\' with the actual class name of the elements you want to find\\ntry:\\n    wait = WebDriverWait(driver, 30) # Wait up to 30 seconds\\n    elements = wait.until(EC.visibility_of_all_elements_located((By.CLASS_NAME, \\'oj-typography-body-md oj-typography-bold\\')))\\n    print(f\"Found {len(elements)} elements with class name \\'your_class_name\\'.\")\\n\\n    # Iterate through the list of elements and extract their text or other attributes\\n    for element in elements:\\n        print(element.text) # Example: Print the text content of each element\\n        # You can also get other attributes like:\\n        # print(element.get_attribute(\\'href\\'))\\n        # print(element.get_attribute(\\'id\\'))\\n\\nexcept Exception as e:\\n    print(f\"Error waiting for elements or extracting data: {e}\")\\n\\n# Example: Extracting text from a specific div element by its class name\\ntry:\\n    div_element = driver.find_element(By.CLASS_NAME, \\'oj-typography-body-md oj-typography-bold\\') # Replace with actual class name\\n    div_text = div_element.text\\n    print(f\"Text from div: {div_text}\")\\nexcept Exception as e:\\n    print(f\"Error extracting text from div: {e}\")\\n'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/72868256/chromedrivermanager-install-doesnt-work-webdriver-manager\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "option = webdriver.ChromeOptions()\n",
    "option.add_argument(\"start-maximized\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()),options=option)\n",
    "#driver.get('https://www.google.com/')\n",
    "driver.get('Provide your URL here, where all job postings along with hiring manager name emails are available')\n",
    "\n",
    "# wait for a minute till the page loads and login happens \n",
    "def countdown(t):\n",
    "    while t:\n",
    "        mins, secs = divmod(t, 60)\n",
    "        timer = '{:02d}:{:02d}'.format(mins, secs)\n",
    "        print(timer, end='\\r')  # Overwrite the line each second\n",
    "        time.sleep(1)\n",
    "        t -= 1\n",
    "\n",
    "    print(\"Fire in the hole!!, login and open the page AND THEN SCROLL DOWN IN BOTH SECTIONS TO LOAD THEM FULLY within 120 seconds\")\n",
    "\n",
    "#t = input(\"Enter the time in seconds: \")\n",
    "t = 120\n",
    "\n",
    "countdown(int(t))\n",
    "\n",
    "\n",
    "# Assuming you've already handled login and are on the target page\n",
    "# Now, retrieve the complete HTML content\n",
    "html_content = driver.page_source\n",
    "#print(html_content)\n",
    "\n",
    "# You can now process this html_content string with a library like Beautiful Soup\n",
    "# (Example using Beautiful Soup)\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find all elements with ID starting with 'opportunities-applied-' using CSS selector\n",
    "elements_with_partial_id = soup.select(\"[id^='opportunities-applied-manager-list-item-layout-']\")\n",
    "print(f\"Found {len(elements_with_partial_id)} elements with ID starting with 'opportunities-applied-manager-list-item-layout-'.\")\n",
    "\n",
    "# Populate 'data_list' with extracted information\n",
    "data_list = []\n",
    "\n",
    "for element in elements_with_partial_id:\n",
    "    #print(f\"ID: {element.get('id')}, Text: {element.get_text(strip=True)}\") # Get text, stripping whitespace\n",
    "    #print(f\"{element.get_text(strip=True)}\") # Get text, stripping whitespace\n",
    "    #data_list.append(element.get_text(strip=True)) # Extract text, stripping whitespace\n",
    "    original_text = element.get_text(strip=True)\n",
    "    cleaned_text = original_text.replace(\"Hiring Manager\", \"\") # Use replace() to remove the text\n",
    "    data_list.append(cleaned_text.strip())  # Append the cleaned text, stripping any extra whitespace\n",
    "\n",
    "#df = pd.DataFrame(data_list)\n",
    "df = pd.DataFrame(data_list, columns=['HiringManagerName'])\n",
    "\n",
    "# Remove duplicate rows based on the 'HiringManagerName' column\n",
    "# By default, keep='first' is used, which keeps the first occurrence of each unique value\n",
    "df = df.drop_duplicates(subset=['HiringManagerName'])\n",
    "\n",
    "# Print the DataFrame (optional)\n",
    "print(\"\\nDataFrame:\")\n",
    "print(df.head())\n",
    "\n",
    "df.to_csv(\"HiringManagerName.csv\", index=False)\n",
    "\n",
    "\n",
    "'''\n",
    "# Example: Extracting the page title\n",
    "page_title = driver.title\n",
    "print(f\"Page Title: {page_title}\")\n",
    "\n",
    "# Assuming you've already handled login and are on the target page\n",
    "\n",
    "# Example: Waiting for an element with a specific ID to be visible\n",
    "# Replace 'your_element_id' with the actual ID of an element on the page\n",
    "try:\n",
    "    wait = WebDriverWait(driver, 30) # Wait up to 20 seconds\n",
    "    element = wait.until(EC.visibility_of_element_located((By.ID, 'opportunities-applied-manager-list-item-layout-activeJobs-4')))\n",
    "    print(f\"Element with ID 'your_element_id' is visible.\")\n",
    "\n",
    "    # Now you can interact with or extract data from the element\n",
    "    print(element.text) # Example: Get the text content of the element\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error waiting for element: {e}\")\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "#FOLLOWING TWO HAS NOT WORKED\n",
    "\n",
    "# Example: Waiting for all elements with a specific class name to be visible\n",
    "# Replace 'your_class_name' with the actual class name of the elements you want to find\n",
    "try:\n",
    "    wait = WebDriverWait(driver, 30) # Wait up to 30 seconds\n",
    "    elements = wait.until(EC.visibility_of_all_elements_located((By.CLASS_NAME, 'oj-typography-body-md oj-typography-bold')))\n",
    "    print(f\"Found {len(elements)} elements with class name 'your_class_name'.\")\n",
    "\n",
    "    # Iterate through the list of elements and extract their text or other attributes\n",
    "    for element in elements:\n",
    "        print(element.text) # Example: Print the text content of each element\n",
    "        # You can also get other attributes like:\n",
    "        # print(element.get_attribute('href'))\n",
    "        # print(element.get_attribute('id'))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error waiting for elements or extracting data: {e}\")\n",
    "\n",
    "# Example: Extracting text from a specific div element by its class name\n",
    "try:\n",
    "    div_element = driver.find_element(By.CLASS_NAME, 'oj-typography-body-md oj-typography-bold') # Replace with actual class name\n",
    "    div_text = div_element.text\n",
    "    print(f\"Text from div: {div_text}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error extracting text from div: {e}\")\n",
    "'''\n",
    "\n",
    "\n",
    "# Don't forget to close the browser when you're done\n",
    "#driver.quit()\n",
    "#print(f\"driver has been quit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "184a9e43-3706-4c1f-bbc8-c7a548a4aa54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "driver has been quit\n",
      "2025-08-01 17:08:18.277772\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "driver.quit()\n",
    "print(f\"driver has been quit\")\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e60cd6-ff74-4eeb-81c6-9d19830e9ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "shivakumar.biradar@Arani.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "358cb20c-0422-470c-a451-cb871ead2ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 203 elements with ID starting with 'opportunities-applied-manager-list-item-layout-'.\n",
      "\n",
      "DataFrame:\n",
      "         HiringManagerName\n",
      "0          Hanrahan, Frank\n",
      "1            Doherty, Alan\n",
      "2  Srinivasan, Kirubakaran\n",
      "3  Narasimhan, Dhananjayan\n",
      "5         Bhandal, Gurmukh\n"
     ]
    }
   ],
   "source": [
    "# You can now process this html_content string with a library like Beautiful Soup\n",
    "# (Example using Beautiful Soup)\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find all elements with ID starting with 'opportunities-applied-' using CSS selector\n",
    "elements_with_partial_id = soup.select(\"[id^='opportunities-applied-manager-list-item-layout-']\")\n",
    "print(f\"Found {len(elements_with_partial_id)} elements with ID starting with 'opportunities-applied-manager-list-item-layout-'.\")\n",
    "\n",
    "# Populate 'data_list' with extracted information\n",
    "data_list = []\n",
    "\n",
    "for element in elements_with_partial_id:\n",
    "    #print(f\"ID: {element.get('id')}, Text: {element.get_text(strip=True)}\") # Get text, stripping whitespace\n",
    "    #print(f\"{element.get_text(strip=True)}\") # Get text, stripping whitespace\n",
    "    #data_list.append(element.get_text(strip=True)) # Extract text, stripping whitespace\n",
    "    original_text = element.get_text(strip=True)\n",
    "    cleaned_text = original_text.replace(\"Hiring Manager\", \"\") # Use replace() to remove the text\n",
    "    data_list.append(cleaned_text.strip())  # Append the cleaned text, stripping any extra whitespace\n",
    "\n",
    "#df = pd.DataFrame(data_list)\n",
    "df = pd.DataFrame(data_list, columns=['HiringManagerName'])\n",
    "\n",
    "# Remove duplicate rows based on the 'HiringManagerName' column\n",
    "# By default, keep='first' is used, which keeps the first occurrence of each unique value\n",
    "df = df.drop_duplicates(subset=['HiringManagerName'])\n",
    "\n",
    "# Print the DataFrame (optional)\n",
    "print(\"\\nDataFrame:\")\n",
    "print(df.head())\n",
    "\n",
    "df.to_csv(\"HiringManagerName.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b13669-6d84-4e9a-bba5-69d919075a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1268c7f5-7f71-4f9c-aaf0-741c825dfc6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
